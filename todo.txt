- Maybe a better TCPConnectionCache can be made by queueing queries to particular addresses in the TCPConnectionCache. That way, we could deal with trying to reestablish connections without worrying about modifying the value while the socket is in use. FOUND ANOTHER WAY

- There is a bug in the routing. The problem seems to be that the lookup() function guarantees different things under different conditions. If the key to be routed to is within the range of the leafset, the node with the closest identifier to the key will be routed to. If there exists an entry in the routing table such that the row is firstDifference(key,self.getIdentifier()) and the column is the int value of key.charAt(row), that node will be routed to. And if neither of those conditions are satisfied, the closest node of all known nodes (using both the routing table and the leaf set) will be routed to. Notice that the second option (checking the routing table) doesn't guarantee that the closest node to the key is routed to. It only guarantees that if there is an entry in the routing table at a particular position, that node will be sent the message. This difference of guarantees is a problem though, because it allows for routing loops, at least, that's what I've discovered using docker to spin up 128 peers. Suppose that a peer receives a message to route to a key. First it checks to see if the key is within the range of the leaf set. Seeing that it isn't, it checks the routing table, and wala! there is an entry at the correct row and column specified above. The receiving node then tries to route the message. It checks the leaf set and there is no match. It checks the routing table and there is no match. After checking both, it checks all nodes it knows about for the closest match, and wala! the closest match happens to be the node it just received the message from. This results in a loop. This can happen because the routing table and the leaf set evolve on their own, with the range of the leaf set becoming smaller and smaller as more and more peers are added, while the routing table might remain quite unchanged, as it has few open slots to place new nodes into. Therefore, a check to the leaf set and a check to a specific routing table position cannot guarantee that we're routing to the closest node we know about. To do that, we need to iterate through the set of all peers we know of (including ourselves) and route to the node with the identifier that is numerically closest to our own. This is counter-intuitive though, as it is a process that doesn't explicitly query the leaf set and the routing table as you'd expect. Instead, the main function of the leaf set and routing table are to LIMIT the number of nodes that we know about over time. Yes, they contain entries that are relevant to routing, but they need to be iterated over together to find the node to be routed to.

